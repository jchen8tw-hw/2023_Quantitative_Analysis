# Question 4 #
##############
library(kmed)
attach(heart)
heart$class = as.factor(ifelse(heart$class==0,0,1))
heart$sex = as.factor(ifelse(heart$sex=='TRUE',1,0))
heart$fbs = as.factor(ifelse(heart$fbs=='TRUE',1,0))
heart$exang = as.factor(ifelse(heart$exang=='TRUE',1,0))
tree.heart=tree(class~.,heart, split = "gini")
#####
# a # +5
#####
library(tree)
tree.heart=tree(class~.,heart, split = "gini")
cv.heart=cv.tree(tree.heart, FUN=prune.misclass ,K = 10)
library(tree)
set.seed(1)
tree.heart=tree(class~.,heart, split = "gini")
set.seed(1)
cv.heart=cv.tree(tree.heart, FUN=prune.misclass ,K = 10)
cv.heart
names(cv.heart)
plot(cv.heart)
cv.heart
rf.2=randomForest(class~.,heart, data=heart, mtry=2,importance=TRUE)
#####
# b # +10
#####
library(randomForest)
set.seed(1)
set.seed(1)
rf.2=randomForest(class~.,heart, data=heart, mtry=2,importance=TRUE)
heart
rf.2=randomForest(class~., data=heart, mtry=2,importance=TRUE)
set.seed(1)
rf.2=randomForest(class~., data=heart, mtry=2,importance=TRUE)
set.seed(1)
rf.4=randomForest(class~., data=heart, mtry=4,importance=TRUE)
set.seed(1)
rf.10=randomForest(class~., data=heart, mtry=10,importance=TRUE)
set.seed(1)
rf.13=randomForest(class~., data=heart, mtry=13,importance=TRUE)
plot(rf.2$err.rate[,1], type = "l", xlab = "Number of Trees", ylab = "OOB error", cex.lab = 1.2, col="blue", ylim=c(0.08, 0.2))
a =length(rf.4$err.rate[,1])
lines(seq(from = 1,to = a, length =a), rf.4$err.rate[,1], col = "red")
a1=length(rf.10$err.rate[,1])
lines(seq(from = 1,to = a1, length =a1), rf.10$err.rate[,1], col = "green")
a2=length(rf.13$err.rate[,1])
lines(seq(from = 1,to = a2, length =a2), rf.13$err.rate[,1], col = "yellow")
legend("topright",legend = c("2", "4","10", "13"),col = c("blue", "red","green", "yellow"), lty = c(1, 1))
plot(rf.2$err.rate[,1], type = "l", xlab = "Number of Trees", ylab = "OOB error", cex.lab = 1.2, col="blue", ylim=c(0.08, 0.2))
a =length(rf.4$err.rate[,1])
lines(seq(from = 1,to = a, length =a), rf.4$err.rate[,1], col = "red")
a1=length(rf.10$err.rate[,1])
lines(seq(from = 1,to = a1, length =a1), rf.10$err.rate[,1], col = "green")
a2=length(rf.13$err.rate[,1])
lines(seq(from = 1,to = a2, length =a2), rf.13$err.rate[,1], col = "yellow")
legend("topright",legend = c("2", "4","10", "13"),col = c("blue", "red","green", "yellow"), lty = c(1, 1))
plot(rf.2$err.rate[,1], type = "l", xlab = "Number of Trees", ylab = "OOB error", cex.lab = 1.2, col="blue")
a =length(rf.4$err.rate[,1])
lines(seq(from = 1,to = a, length =a), rf.4$err.rate[,1], col = "red")
a1=length(rf.10$err.rate[,1])
lines(seq(from = 1,to = a1, length =a1), rf.10$err.rate[,1], col = "green")
a2=length(rf.13$err.rate[,1])
lines(seq(from = 1,to = a2, length =a2), rf.13$err.rate[,1], col = "yellow")
legend("topright",legend = c("2", "4","10", "13"),col = c("blue", "red","green", "yellow"), lty = c(1, 1))
#####
# a # +5
#####
library(MASS)
library(MASS)
set.seed(1)
lm.fit1 = glm(medv ~. Boston)
Boston[-medv]
Boston[,-medv]
attach(Boston)
Boston[-medv]
library(MASS)
attach(Boston)
grid = seq(0,1,length = 1000)
set.seed(1)
lasso.fit = glmnet(Boston[-medv],medv,alpha = 1, lambda = grid)
set.seed(1)
lasso.kfold = cv.glmnet(Boston[-medv],medv,alpha=1, nfolds=10, lambda = grid)
best.s = lasso.kfold$lambda.min
best.s
library(glmnet)
lasso.fit = glmnet(Boston[-medv],medv,alpha = 1, lambda = grid)
set.seed(1)
lasso.kfold = cv.glmnet(Boston[-medv],medv,alpha=1, nfolds=10, lambda = grid)
X=Boston[-medv]
View(X)
View(X)
library(MASS)
attach(Boston)
data=Boston
View(data)
X = Boston[,-medv]
View(X)
X = data[,-14]
View(X)
Y = data[,14]
grid = seq(0,1,length = 1000)
set.seed(1)
lasso.fit = glmnet(X,Y,alpha = 1, lambda = grid)
set.seed(1)
lasso.kfold = cv.glmnet(X,Y,alpha=1, nfolds=10, lambda = grid)
best.s = lasso.kfold$lambda.min
best.s
lasso.kfold = cv.glmnet(X[,1:13],Y,alpha=1, nfolds=10, lambda = grid)
grid = seq(0,1,length = 100)
set.seed(1)
lasso.fit = glmnet(X,Y,alpha = 1, lambda = grid)
Y = dataframe(data[,14])
Y = data[,14].dataframe
Y = data.frame(data[,14])
lasso.kfold = cv.glmnet(X[,1:13],Y,alpha=1, nfolds=10, lambda = grid)
View(Y)
View(Y)
grid = seq(0,100,length = 1000)
lasso.fit = glmnet(X,Y,alpha = 1, lambda = grid) # "alpha" = 0 for ridge regression, =1 for Lasso
X = model.matrix(medv~.,Boston)[,-1] #design matrix without intercept.
Y= Boston$medv
View(X)
grid = seq(0,1,length = 100)
set.seed(1)
lasso.fit = glmnet(X,Y,alpha = 1, lambda = grid)
set.seed(1)
lasso.kfold = cv.glmnet(X,Y,alpha=1, nfolds=10, lambda = grid)
best.s = lasso.kfold$lambda.min
best.s
library(MASS)
attach(Boston)
library(glmnet)
X = model.matrix(medv~.,Boston)[,-1] #design matrix without intercept.
Y= Boston$medv
library(glmnet)
grid = seq(0,1,length = 100)
set.seed(1)
lasso.fit = glmnet(X,Y,alpha = 1, lambda = grid)
set.seed(1)
lasso.kfold = cv.glmnet(X,Y,alpha=1, nfolds=10, lambda = grid)
best.s = lasso.kfold$lambda.min
best.s
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
predict(lasso.fit, type = "coefficients", s=best.s)
fit = lm(medv~crim+zn+chas+nox+rm+dis+rad+tax+ptratio+black+lstat,subset = train)
mean((predict(fit, Boston) - medv)[-train]^2)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
fit = lm(medv~crim+zn+chas+nox+rm+dis+rad+tax+ptratio+black+lstat,subset = train)
mean((predict(fit, Boston) - medv)[-train]^2)
data = Boston
set.seed(1)
index = sample(1:nrow(Boston), nrow(Boston)/2)
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
set.seed(1)
train_ <- scaled[index,]
test_ <- scaled[-index,]
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(formula=f,data=train_,hidden=c(5,4,3,2),linear.output=T)
windows()
plot(nn)
pr.nn <- compute(x = nn, covariate = test_[,2:8])
pr.nn_ <- pr.nn$net.result*( max(data$medv)-min(data$medv) ) + min(data$medv)
MSE.nn <- sum((test$medv - pr.nn_)^2)/nrow(test)
MSE.nn
n <- names(data)
pr.nn <- compute(x = nn, covariate = test_[,2:8])
data = Boston
n <- names(data)
set.seed(1)
index = sample(1:nrow(Boston), nrow(Boston)/2)
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
set.seed(1)
train_ <- scaled[index,]
test_ <- scaled[-index,]
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(formula=f,data=train_,hidden=c(5,4,3,2),linear.output=T)
windows()
plot(nn)
pr.nn <- compute(x = nn, covariate = test_[,2:8])
pr.nn_ <- pr.nn$net.result*( max(data$medv)-min(data$medv) ) + min(data$medv)
MSE.nn <- sum((test$medv - pr.nn_)^2)/nrow(test)
MSE.nn
rm(list=ls(all=T))
##############
# Question 1 #
##############
#####
# a #
#####
library(ISLR)
data = Carseats
n <- names(data)
data = Carseats[n[!n %in% c("ShelveLoc","Urban","US")]]
set.seed(1234)
index <- sample(1:nrow(data),round(0.8*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(Sales~., data=train)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$Sales)^2)/nrow(test)
# MSE = 4.419823
#####
# b #
#####
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
set.seed(1234)
train_ <- scaled[index,]
test_ <- scaled[-index,]
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("Sales ~", paste(n[!n %in% "Sales"], collapse = " + ")))
nn <- neuralnet(formula=f,data=train_,hidden=c(5,3,2),linear.output=T)
windows()
plot(nn)
pr.nn <- compute(x = nn, covariate = test_[,2:8])
pr.nn_ <- pr.nn$net.result*( max(data$Sales)-min(data$Sales) ) + min(data$Sales)
MSE.nn <- sum((test$Sales - pr.nn_)^2)/nrow(test)
MSE.nn
library(MASS)
attach(Boston)
#####
# c # +5
#####
data = Boston
n <- names(data)
set.seed(1)
index = sample(1:nrow(Boston), nrow(Boston)/2)
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
set.seed(1)
train_ <- scaled[index,]
test_ <- scaled[-index,]
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(formula=f,data=train_,hidden=c(5,4,3,2),linear.output=T)
windows()
plot(nn)
pr.nn <- compute(x = nn, covariate = test_[,2:8])
View(test_)
pr.nn <- compute(x = nn, covariate = test_[,1:13])
pr.nn_ <- pr.nn$net.result*( max(data$medv)-min(data$medv) ) + min(data$medv)
MSE.nn <- sum((test$medv - pr.nn_)^2)/nrow(test)
MSE.nn
MSE.nn <- sum((test_$medv - pr.nn_)^2)/nrow(test_)
MSE.nn
library(glmnet)
rm(list=ls(all=T))
n = 55
R = 100
grid = seq(0,2,length = 100)
Result_lasso_1 = 0
Lambda_1 = 0
count = NULL
summary.fit = NULL
for (r in 1:R) {
# generate dataset X
X = matrix(nrow = n, ncol = 50)
set.seed(r)
X = matrix(rnorm(55*50),nrow = 55)
# generate residuals
set.seed(r)
e = rnorm(n)
y_DGP1 = 2 + X[,1] + X[,2] + e
data = data.frame(X,y_DGP1)
lasso.fit_1 = glmnet(X[,1:50],y_DGP1, alpha = 1, lambda = grid)
Result_lasso_1 = Result_lasso_1 + (1/R)*coef(lasso.fit_1)[2:51,]
if (coef(lasso.fit_1)[2,] != 0){
count[r]=1
}else{
count[r]=0
}
lasso.kfold_1 = cv.glmnet(X[,1:50],y_DGP1,alpha=0, nfolds=10, lambda = grid)
best.s1 = lasso.kfold_1$lambda.min
Lambda_1 = Lambda_1 + (1/R)*best.s1
backward.fit = regsubsets(y_DGP1~.,data,nvmax=50,method = "backward")
a = which.min(summary(backward.fit)$bic)
summary.fit[r] = coef(backward.fit,a)
}
sum(count)/100
set.seed(1)
X = rnorm(100,10,1)
set.seed(2)
Y = rnorm(100,10,1)
set.seed(1)
Z = NULL
for (i in 1:10000) {
set.seed(i)
index = sample(100,100, replace = T)
X_bar = mean(X[index])
Y_bar = mean(Y[index])
Z[i] = X_bar^5 / (X_bar+Y_bar)^2
}
sd(Z)
library(leaps)
backward.fit = regsubsets(y_DGP1~.,data,nvmax=50,method = "backward")
summary(backward.fit)
a = which.min(summary(backward.fit)$bic)
summary.fit = coef(backward.fit,a)
summary.fit
summary.fit[1,2]
library(kmed)
attach(heart)
heart$class = as.factor(ifelse(heart$class==0,0,1))
heart$sex = as.factor(ifelse(heart$sex=='TRUE',1,0))
heart$fbs = as.factor(ifelse(heart$fbs=='TRUE',1,0))
heart$exang = as.factor(ifelse(heart$exang=='TRUE',1,0))
#####
# a # +5
#####
library(tree)
set.seed(1)
tree.heart=tree(class~.,heart, split = "gini")
set.seed(1)
cv.heart=cv.tree(tree.heart, FUN=prune.misclass ,K = 10)
cv.heart
names(cv.heart)
plot(cv.heart)
set.seed(1)
cv.heart=cv.tree(tree.heart,K = 10)
cv.heart
names(cv.heart)
plot(cv.heart)
which.min(cv.heart$dev)
library(tree)
set.seed(1)
tree.heart=tree(class~.,heart, split = "gini")
set.seed(1)
cv.heart=cv.tree(tree.heart, FUN=prune.misclass ,K = 10)
which.min(cv.heart$dev)
cv.heart
library(tree)
set.seed(1)
tree.heart=tree(class~.,heart, split = "gini")
set.seed(1)
cv.heart=cv.tree(tree.heart, FUN=prune.misclass ,K = 10)
cv.heart
names(cv.heart)
plot(cv.heart)
library(glmnet)
rm(list=ls(all=T))
n = 55
R = 100
grid = seq(0,2,length = 100)
Result_lasso_1 = 0
Lambda_1 = 0
count = NULL
summary.fit = NULL
ols= NULL
for (r in 1:R) {
# generate dataset X
X = matrix(nrow = n, ncol = 50)
set.seed(r)
X = matrix(rnorm(55*50),nrow = 55)
# generate residuals
set.seed(r)
e = rnorm(n)
y_DGP1 = 2 + X[,1] + X[,2] + e
data = data.frame(X,y_DGP1)
lasso.fit_1 = glmnet(X[,1:50],y_DGP1, alpha = 1, lambda = grid)
Result_lasso_1 = Result_lasso_1 + (1/R)*coef(lasso.fit_1)[2:51,]
if (coef(lasso.fit_1)[2,] != 0){
count[r]=1
}else{
count[r]=0
}
lasso.kfold_1 = cv.glmnet(X[,1:50],y_DGP1,alpha=0, nfolds=10, lambda = grid)
best.s1 = lasso.kfold_1$lambda.min
Lambda_1 = Lambda_1 + (1/R)*best.s1
backward.fit = regsubsets(y_DGP1~.,data,nvmax=50,method = "backward")
a = which.min(summary(backward.fit)$bic)
summary.fit[r] = coef(backward.fit,a)
ols[r]=coef(lm(y_DGP1~X[,1:50])[1,1]
}
for (r in 1:R) {
# generate dataset X
X = matrix(nrow = n, ncol = 50)
set.seed(r)
X = matrix(rnorm(55*50),nrow = 55)
# generate residuals
set.seed(r)
e = rnorm(n)
y_DGP1 = 2 + X[,1] + X[,2] + e
data = data.frame(X,y_DGP1)
lasso.fit_1 = glmnet(X[,1:50],y_DGP1, alpha = 1, lambda = grid)
Result_lasso_1 = Result_lasso_1 + (1/R)*coef(lasso.fit_1)[2:51,]
if (coef(lasso.fit_1)[2,] != 0){
count[r]=1
}else{
count[r]=0
}
lasso.kfold_1 = cv.glmnet(X[,1:50],y_DGP1,alpha=0, nfolds=10, lambda = grid)
best.s1 = lasso.kfold_1$lambda.min
Lambda_1 = Lambda_1 + (1/R)*best.s1
backward.fit = regsubsets(y_DGP1~.,data,nvmax=50,method = "backward")
a = which.min(summary(backward.fit)$bic)
summary.fit[r] = coef(backward.fit,a)
ols[r]=coef(lm(y_DGP1~X)[1,1]
}
library(glmnet)
rm(list=ls(all=T))
n = 55
R = 100
grid = seq(0,2,length = 100)
Result_lasso_1 = 0
Lambda_1 = 0
count = NULL
summary.fit = NULL
ols= NULL
for (r in 1:R) {
# generate dataset X
X = matrix(nrow = n, ncol = 50)
set.seed(r)
X = matrix(rnorm(55*50),nrow = 55)
# generate residuals
set.seed(r)
e = rnorm(n)
y_DGP1 = 2 + X[,1] + X[,2] + e
data = data.frame(X,y_DGP1)
lasso.fit_1 = glmnet(X[,1:50],y_DGP1, alpha = 1, lambda = grid)
Result_lasso_1 = Result_lasso_1 + (1/R)*coef(lasso.fit_1)[2:51,]
if (coef(lasso.fit_1)[2,] != 0){
count[r]=1
}else{
count[r]=0
}
lasso.kfold_1 = cv.glmnet(X[,1:50],y_DGP1,alpha=0, nfolds=10, lambda = grid)
best.s1 = lasso.kfold_1$lambda.min
Lambda_1 = Lambda_1 + (1/R)*best.s1
backward.fit = regsubsets(y_DGP1~.,data,nvmax=50,method = "backward")
ols[r] = coef(lm(y_DGP1~X))[2,2]
a = which.min(summary(backward.fit)$bic)
summary.fit[r] = coef(backward.fit,a)
}
X = matrix(nrow = n, ncol = 50)
set.seed(r)
X = matrix(rnorm(55*50),nrow = 55)
# generate residuals
set.seed(1)
e = rnorm(n)
y_DGP1 = 2 + X[,1] + X[,2] + e
data = data.frame(X,y_DGP1)
lasso.fit_1 = glmnet(X[,1:50],y_DGP1, alpha = 1, lambda = grid)
Result_lasso_1 = Result_lasso_1 + (1/R)*coef(lasso.fit_1)[2:51,]
if (coef(lasso.fit_1)[2,] != 0){
count[1]=1
}else{
count[1]=0
}
lasso.kfold_1 = cv.glmnet(X[,1:50],y_DGP1,alpha=0, nfolds=10, lambda = grid)
best.s1 = lasso.kfold_1$lambda.min
Lambda_1 = Lambda_1 + (1/R)*best.s1
backward.fit = regsubsets(y_DGP1~.,data,nvmax=50,method = "backward")
ols[1] = coef(lm(y_DGP1~X))[2,2]
ols[1] = coef(lm(y_DGP1~X))
ols = coef(lm(y_DGP1~X))
ols = coef(lm(y_DGP1~X))[2]
ols
library(glmnet)
rm(list=ls(all=T))
n = 55
R = 100
grid = seq(0,2,length = 100)
Result_lasso_1 = 0
Lambda_1 = 0
count = NULL
summary.fit = NULL
ols= NULL
for (r in 1:R) {
# generate dataset X
X = matrix(nrow = n, ncol = 50)
set.seed(r)
X = matrix(rnorm(55*50),nrow = 55)
# generate residuals
set.seed(r)
e = rnorm(n)
y_DGP1 = 2 + X[,1] + X[,2] + e
data = data.frame(X,y_DGP1)
lasso.fit_1 = glmnet(X[,1:50],y_DGP1, alpha = 1, lambda = grid)
Result_lasso_1 = Result_lasso_1 + (1/R)*coef(lasso.fit_1)[2:51,]
if (coef(lasso.fit_1)[2,] != 0){
count[r]=1
}else{
count[r]=0
}
lasso.kfold_1 = cv.glmnet(X[,1:50],y_DGP1,alpha=0, nfolds=10, lambda = grid)
best.s1 = lasso.kfold_1$lambda.min
Lambda_1 = Lambda_1 + (1/R)*best.s1
backward.fit = regsubsets(y_DGP1~.,data,nvmax=50,method = "backward")
ols[r] = coef(lm(y_DGP1~X))[2]
a = which.min(summary(backward.fit)$bic)
summary.fit[r] = coef(backward.fit,a)
}
#####
# c # +5
#####
ols
